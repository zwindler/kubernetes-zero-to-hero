---
marp: true
theme: gaia
markdown.marp.enableHtml: true
paginate: true
---

<style>

section {
  background-color: #fefefe;
  color: #333;
}

img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
blockquote {
  background: #ffedcc;
  border-left: 10px solid #d1bf9d;
  margin: 1.5em 10px;
  padding: 0.5em 10px;
}
blockquote:before{
  content: unset;
}
blockquote:after{
  content: unset;
}
</style>

<!-- _class: lead -->

# Module 4 : Gestion du cycle de vie des applications

*Formation Kubernetes - D√©butant √† Avanc√©*

---

## Plan du Module 4

**Partie 1**
Health checks, gestion des ressources, types de containers

**Partie 2**
M√©triques syst√®me, logs, utilisation de Prometheus/Grafana

**Partie 3**
HPA, VPA, outils de scaling avanc√©s

**Partie 4**
Rolling updates, Blue/Green, Canary, Rollback

---

<!-- _class: lead -->

# Partie 1 : apps production-ready
## Pr√©parer vos applications pour la production

---

<!-- _class: lead -->

# startup / liveness / readiness probes

---
## Pourquoi rajouter des sondes ?

**Sans sondes :**
- Le kubelet s'assure que le container est *vivant*, c'est √† dire, que son processus principal est *vivant*

Mais *vivant* ne veut pas dire fonctionnel ! Ex :

- processus principal fig√© mais non fonctionnel
- serveur pas pr√™t √† recevoir du trafic
- processus enfant n√©cessaire, mais mort


---

## Les diff√©rentes Probes de Kubernetes

Kubernetes ajoute un certain nombre de sondes (probes) visant √† r√©pondre √† des questions diverses :

- **Startup Probe** : le container a-t-il fini de d√©marrer ?
- **Liveness Probe** : est ce que le container fonctionne r√©ellement ?
- **Readiness Probe** : est ce que le container est pr√™t √† recevoir du trafic ?

Ces startup / readiness / liveness probes sont g√©r√©es par **container**.

---

## Les diff√©rents types d'actions des **Probes**

[Probe (v1 core)](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.33/#probe-v1-core) est une des API de Kubernetes

Il existe 4 grands types de probes aujourd'hui :

- **HTTP** - la plus courante, effectue un call sur un serveur HTTP
- **TCP** - dans le cas o√π un call HTTP n'est pas possible
- **gRPC** - pour les serveurs gRPC
- **exec** - ex√©cute une commande arbitraire sur le container quand celui ci ne contient pas de serveur

---

## Startup Probe : g√©rer les d√©marrages lents

Inhibe les autres sondes du container, jusqu'√† ce que l'application soit d√©marr√©e.

**Cas d'usage :** apps avec un temps de d√©marrage long **et** variable.

Trivia : arriv√©es un peu apr√®s les autres (API `beta` en 1.18 / 2020)

---

## Startup Probe : exemple de manifest

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-with-startup
spec:
  containers:
  - name: slow-app
    image: myapp:1.0
    ports:
    - containerPort: 8080
    startupProbe:
      httpGet:
        path: /startup
        port: 8080
      periodSeconds: 10
      failureThreshold: 30  #<===== 30 * 10s = 5 minutes max                              
```

---

## Liveness Probe

Red√©marre le conteneur si la sonde √©choue

```yaml
  [...]
  containers:
  - name: web-app
    livenessProbe:
      httpGet:
        path: /health
        port: 80
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3
```

---

## Readiness Probe

Retire le Pod du Service si la sonde √©choue (sans le red√©marrer)

```yaml
  [...]
  containers:
  - name: web-app
    image: myapp:1.0
    ports:
    - containerPort: 8080
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
      periodSeconds: 5
      failureThreshold: 3
```

---

## Configuration des sondes : param√®tres

```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30    # D√©lai avant la premi√®re v√©rification
  periodSeconds: 10          # Fr√©quence des v√©rifications
  timeoutSeconds: 5          # Timeout par requ√™te
  successThreshold: 1        # Succ√®s cons√©cutifs pour consid√©rer OK
  failureThreshold: 3        # √âchecs cons√©cutifs pour consid√©rer KO
```

---

## Exemple Python/Flask

```python
@app.route('/health')
def health():
    # V√©rifications basiques (pas de d√©pendances externes)
    return {'status': 'healthy'}, 200

@app.route('/ready')
def ready():
    # V√©rification 
    if database.pool_is_available:
        return {'status': 'ready'}, 200
    else:
        return {'status': 'not ready'}, 503

@app.route('/startup')
def startup():
    # V√©rifications de d√©marrage (migrations, init)                                                    
    if app.is_initialized():
        return {'status': 'started'}, 200
    else:
        return {'status': 'starting'}, 503
```

---

## Types de probes : autre exemples

```yaml
# Probe TCP - ex: pour bases de donn√©es
# Probe gRPC (Kubernetes 1.23+)
livenessProbe:
  tcpSocket: #remplacer `tcpSocket:` par `grpc:` si on souhaite une sonde grpc        
    port: 5432
  initialDelaySeconds: 30
  periodSeconds: 10

# Probe exec - commande personnalis√©e
livenessProbe:
  exec:
    command:
    - /bin/sh
    - -c
    - "pg_isready -U postgres"
  initialDelaySeconds: 30
  periodSeconds: 10
```

---

<!-- _class: lead -->

# Gestion des ressources

---

## Requests et Limits

k8s est un orchestrateur de containers, **mais il a besoin d'aide** !

Dans nos manifests YAML, on va d√©finir les besoins (requests) et les limites (limits) de nos containers.

```yaml
  [...]
  containers:
  - name: container1
    resources:
      requests:        # Ressources demand√©es (au scheduling)
        memory: "256Mi"
        cpu: "250m"
      limits:          # Limites maximales (impos√©es au runtime)                        
        memory: "512Mi"
        cpu: "500m"
```

---

## Unit√©s des ressources (1/2)

**CPU (`cpu:`) :**
- `1` ou `1000m` = 1 coeur de CPU 
- `100m` = 1 dixi√®me de coeur de CPU

**M√©moire (`memory:` ou `hugepages-<size>:`) :**
- `128Mi` = 128 * 1024 * 1024 bytes
- `128M` = 128 * 1000 * 1000 bytes

---

## Unit√©s de ressources (2/2)

Plus anecdotique, la taille allou√©e aux modifications √©ph√©m√®res sur le FS du container `ephemeral-storage:` :

**Stockage √©ph√©m√®re :**
- `2Gi` = 2 * 1024¬≥ bytes
- `1500M` = 1500 * 1000¬≤ bytes

---

## Quality of Service (QoS) (1/2)

Kubernetes classe les Pods selon leurs ressources CPU et m√©moire.

En cas de pression sur la m√©moire sur un Node, la QoS d√©termine dans quel ordre les Pods seront √©vinc√©s du Node.

**Pods prioritaires pour QoS Guaranteed :**
- Applications critiques (bases de donn√©es, API principales)
- Services syst√®me essentiels (DNS, ingress controllers)
- Workloads sensibles aux variations de performance

---

## Quality of Service (QoS) (2/2)

**Guaranteed**
- **Tous** les conteneurs ont des requests et limits. Requests = limits.

**Burstable**
- Au moins un des conteneur a requests ou limits. Requests ‚â† limits.

**BestEffort**
- Aucun requests ni limits d√©fini

---

<!-- _class: lead -->

# Diff√©rents types de containers

---

## Init Containers

Conteneurs qui s'ex√©cutent **avant** les conteneurs principaux.

Peuvent √™tre utiles dans les cas o√π il est n√©cessaire de pr√©parer l'environnement pour l'application : 

- changement des permissions sur un volume
- initialisation d'une base de donn√©es
- t√©l√©chargement de ressources externes
- ...

---

## Init containers : exemple de manifest

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-with-init
spec:
  initContainers:
  - name: config-setup
    image: busybox:1.37
    command: ['sh', '-c', 'cp /config-templates/* /shared-config/']                                         
    volumeMounts:
    - name: shared-config
      mountPath: /shared-config
  containers:
  - name: web-app
    image: myapp:1.0
    volumeMounts:
    - name: shared-config
      mountPath: /app/config
  volumes:
  - name: shared-config
    emptyDir: {}
```

---

## Sidecar Containers

Fonctionnalit√© "relativement r√©cente" (`alpha` en 1.28, `stable` depuis Kubernetes v1.33) :
- init containers *sp√©cial*, `restartPolicy: Always` qui subsistent apr√®s le d√©marrage du container principal.

Utiles pour certains proxy de DB externes (la connexion doit √™tre disponible au boot de l'application principale).

‚ö†Ô∏è Ne pas confondre avec le *pattern* sidecar container (container qui √©tend les fonctionnalit√©s d'un Pod pour du logging, monitoring...)

---


## Sidecar Containers : exemple de manifests

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: sidecar-cronjob
spec:
  schedule: "* * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: sidecar-user
            image: zwindler/sidecar-user
          initContainers:
          - name: slow-sidecar
            image: zwindler/slow-sidecar                                                                    
            restartPolicy: Always
          restartPolicy: Never
```

[blog.zwindler.fr - Kubernetes 1.29 - sidecar container](https://blog.zwindler.fr/2024/07/19/kubernetes-1-29-sidecar-containers/)

---

## Bonnes pratiques pour la production (1/2)

**Probes :**

- ‚úÖ Toujours impl√©menter liveness ET readiness probes
- ‚úÖ Endpoints l√©gers et rapides (< 1s)
- ‚úÖ Diff√©rencier `/health` (basique) et `/ready` (complet)
- ‚ùå Ne jamais v√©rifier les d√©pendances externes dans liveness (incidents en casquade)

---

## Bonnes pratiques pour la production (2/2)

**Limits et requests :**

- ‚úÖ Toujours d√©finir `requests` cpu et m√©moire
  - cible = consommation moyenne / habituelle
- ‚úÖ Toujours d√©finir `limits` pour la **m√©moire**
  - pr√©voir assez de marge pour les "bursts"
- ‚ùå Jamais de `limits` **cpu** pour les applications sensibles √† la latence (cf [Stop Using CPU Limits on Kubernetes](https://home.robusta.dev/blog/stop-using-cpu-limits))
- QoS `Guaranteed` seulement pour les applications **tr√®s** critiques

---

<!-- _class: lead -->

# Partie 2 : Observabilit√©
## Surveiller et diagnostiquer vos applications

---

## Les 3 piliers de l'observabilit√©

L'observabilit√© moderne repose sur 3 types de t√©l√©m√©trie compl√©mentaires :

- **üìä M√©triques** : Donn√©es num√©riques agr√©g√©es dans le temps (CPU, RAM, compteurs...)
- **üìù Logs** : Messages textuels horodat√©s des applications et syst√®mes
- **üîç Traces** : Suivi des requ√™tes √† travers les microservices

---

## M√©triques syst√®me avec metrics-server

Composant optionnel (mais courant) qui collecte les *m√©triques* de base CPU/M√©moire des Nodes et Pods

```bash
# Installation (si pas d√©j√† pr√©sent)
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# V√©rifier l'installation
kubectl get deployment metrics-server -n kube-system

# Voir les m√©triques des n≈ìuds
kubectl top nodes

# Voir les m√©triques des pods
kubectl top pods
kubectl top pods --containers  # Par conteneur
kubectl top pods --sort-by=cpu
```

---

## Prometheus : collecteur de m√©triques

Historiquement les outils de collecte de *m√©triques* se r√©partissent entre 2 philosophies, **Push** (passif) vs **Pull** (actif)

Prometheus utilise majoritairement l'architecture **Pull** :

- **Service Discovery** automatique des ressources Kubernetes
- **Scraping** des m√©triques des applications
- **Stockage** local en time-series database
<br/>

**Alternatives :** [Thanos](https://thanos.io/), [Mimir](https://grafana.com/oss/mimir/), [VictoriaMetrics](https://victoriametrics.com/)

---

## Exporters

Pour collecter des m√©triques k8s, Prometheus utilise des **exporters**. Voici quelques exemples utiles :

- **[cadvisor](https://github.com/google/cadvisor)** : m√©triques des containers (CPU/RAM/r√©seau) - *int√©gr√© dans kubelet*
- **[kube-state-metrics](https://github.com/kubernetes/kube-state-metrics)** : √©tat des objets K8s (pods, deployments...) 
- **[node-exporter](https://github.com/prometheus/node_exporter)** : m√©triques syst√®me des Nodes (disque, r√©seau, OS)
- **[enix/x509-certificate-exporter](https://github.com/enix/x509-certificate-exporter)** : exporter permettant de surveiller divers types de certificats dans le cluster

---

## Quelques m√©triques Kubernetes utiles dans Prometheus

- `up` : services qui r√©pondent
- `kube_pod_status_phase` : √©tats des pods
- `kube_deployment_status_replicas` : r√©plicas des deployments
- `container_memory_usage_bytes` : utilisation m√©moire par container
- `container_cpu_usage_seconds_total` : utilisation CPU par container

---

## PromQL dans Prometheus (1/2)

On peut ex√©cuter des requ√™tes PromQL directement depuis l'interface de Prometheus

```promql
# Pods en cours d'ex√©cution
sum by (namespace) (kube_pod_status_phase{phase="Running"})

# Top 10 des pods qui consomment le plus de CPU
topk(10, rate(container_cpu_usage_seconds_total[5m]))

# Utilisation m√©moire par namespace
sum by (namespace) (container_memory_usage_bytes{container!="POD"})

# Pods qui red√©marrent
increase(kube_pod_container_status_restarts_total[1h]) > 0
```

---

## PromQL dans Prometheus (2/2)

TODO capture d'√©cran d'exemple

---

## Requ√™tes dans Grafana

TODO capture d'√©cran d'exemple bis

---

## Collecte de logs dans Kubernetes

Solutions d'agr√©gation de *logs* :

- **[Grafana Loki](https://grafana.com/oss/loki/)** : "Prometheus pour les logs"
- **[Elastic Stack](https://www.elastic.co/elastic-stack)** : Elasticsearch + Logstash + Kibana
- **[VictoriaLogs](https://docs.victoriametrics.com/victorialogs/)** : Performance optimis√©e
- **[Quickwit](https://quickwit.io/)** : Search engine moderne pour logs


**Pattern commun :** un agent sur chaque n≈ìud collecte puis envoi vers stockage central ‚Üí Interface de recherche

---

## Backends pour les traces distribu√©es


Suivre une requ√™te utilisateur √† travers tous les microservices (et les fonctions d'un m√™me microservice) pour identifier les goulots d'√©tranglement.

Solutions de collecte de *traces* :

- **[Jaeger](https://www.jaegertracing.io/)** : CNCF graduated
- **[Zipkin](https://zipkin.io/)** : Historique, compatible avec Jaeger
- **[Tempo](https://grafana.com/oss/tempo/)** : Backend Grafana Labs pour les traces


---

## OpenTelemetry : le standard unifi√©

**OpenTelemetry** unifie la collecte des 3 types de t√©l√©m√©trie :

TODO je vais faire un sch√©ma pour expliquer tout √ßa

Otel (abr√©viation usuelle) est le standard de facto, permet d'avoir SDK unique dans le code tout en offrant la flexibilit√© des backends de stockage.

Plus d'infos : [opentelemetry.io](https://opentelemetry.io/)

---

<!-- _class: lead -->

# Partie 3 : Scaling et optimisation
## Adapter automatiquement vos ressources

---

## Horizontal Pod Autoscaler (HPA)

Mise √† l'√©chelle automatique du nombre de pods selon les m√©triques de base (consommation CPU / RAM des replicas)

Pr√©requis :
- **metrics-server** install√©
- un Deployment avec `resources.requests` d√©finis

---

## HPA : exemple de manifest

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70                                                               
```

---

## HPA avec m√©triques *custom*

Au-del√† des indicateurs basiques  CPU/RAM, HPA peut (en th√©orie) scaler sur **n'importe quelle m√©trique**, notamment Prometheus.

En pratique, √ßa demande un peu de configuration et un composant additionnel appel√© [Prometheus Adapter](https://github.com/kubernetes-sigs/prometheus-adapter) et pas mal de configuration

<br/>

- [deezer.io - Optimizing Kubernetes resources with Horizontal Pod Autoscaling via Custom Metrics and the Prometheus Adapter](https://deezer.io/optimizing-kubernetes-resources-with-horizontal-pod-autoscaling-via-custom-metrics-and-the-a76c1a66ff1c)
- [blog.zwindler.fr - le m√™me article, mais en fran√ßais ;-P](https://blog.zwindler.fr/2024/10/11/optimisation-ressources-kubernetes-autoscaling-horizontal-custom-metrics-prometheus-adapter/)


---

## Vertical Pod Autoscaler (VPA)

Ajuste automatiquement et √† la vol√©e les requests/limits des conteneurs

Le VPA est disponibles dans plusieurs **Modes** :
- **Off** : Analyse uniquement, pas de modification
- **Recreation** : Supprime et recr√©e les pods avec nouvelles ressources
- **Auto** : Met √† jour automatiquement (si possible sans interruption)

---

## VPA : exemple de manifest

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: webapp-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp
  updatePolicy:
    updateMode: "Auto"        # Auto, Recreation, Off
  resourcePolicy:
    containerPolicies:
    - containerName: webapp
      maxAllowed:
        cpu: 1
        memory: 2Gi
      minAllowed:
        cpu: 100m
        memory: 128Mi
```

---

## HPA vs VPA : quand utiliser quoi ?

Horizontal Pod Autoscaler (HPA) :

- Applications stateless, avec charge variable dans le temps
- Peut scaler rapidement, mais pas de "scale-to-zero"

Vertical Pod Autoscaler (VPA) :

- Utile pour optimiser l'utilisation des ressources
- Peut n√©cessiter des red√©marrages

**Dans la plupart des cas, on utilisera pas le VPA**

---

## Bonnes pratiques HPA / VPA

**HPA :**

- √âviter les oscillations : utiliser `scaleUpPolicy`/`ScaleDownPolicy`
- Utilisez des m√©triques custom (ex: requ√™tes/sec, taille des queues)
- ‚ö†Ô∏è Attention aux cold starts : `initialDelaySeconds` sur les probes

**VPA :**

- Mode `Off` d'abord pour analyser sans risque
- D√©finir `minAllowed`/`maxAllowed` pour √©viter les extr√™mes
- ‚ùå Ne **jamais** utiliser HPA + VPA sur la m√™me ressource

---

## Outils de scaling avanc√©s

**[KEDA](https://keda.sh/)** (Kubernetes Event-driven Autoscaling) :
- HPA am√©lior√©s : scale sur n'importe quelle m√©trique facilement
- Connecteurs pour Kafka, Redis, PostgreSQL, AWS SQS...
- Scale-to-zero pour √©conomiser les ressources

**[KNative](https://knative.dev/)** (Kubernetes Native Serverless) :
- Platform serverless sur Kubernetes
- Auto-scaling agressif avec scale-to-zero
- *Request-driven scaling*

---

## Outils d'optimisation des ressources (1/2)

Les limits / requests sont dures √† estimer √† l'√©chelle. Pour √©viter les crashs et les gaspillages : utiliser des outils du march√© !

**[Goldilocks](https://goldilocks.docs.fairwinds.com/)** :
- Utilise les VPA pour faire des recommendations apr√®s une phase d'apprentissage, dans une interface web

---

## Outils d'optimisation des ressources (2/2)

**[KRR](https://github.com/robusta-dev/krr)** (Kubernetes Resource Recommender) :
- CLI pour analyser l'utilisation des ressources dans le temps
- Recommandations bas√©es sur des m√©triques Prometheus


**[Kubecost](https://www.kubecost.com/)** :
- Analyse des co√ªts par namespace, workload, √©quipe
- Optimisation financi√®re des ressources

---

<!-- _class: lead -->

# Partie 4 : Strat√©gies de mise √† jour
## D√©ployer et mettre √† jour en production

---

## Strat√©gies de d√©ploiement Kubernetes

Plusieurs "pattern" usuels pour mettre √† jour des apps dans k8s. 

Certains sont pr√©sents par d√©faut dans les Deployments, d'autres non.

- **RollingUpdate** : mise √† jour progressive, sans interruption
- **Recreate** : arr√™t complet puis red√©marrage avec la nouvelle version
- **Blue/Green** : bascule compl√®te entre deux environnements d√©ploy√©s en parall√®le
- **Canary** : Test progressif sur un sous-ensemble de requ√™tes

---

## `RollingUpdate` : mise √† jour progressive (1/2)

**Strat√©gie par d√©faut** des Deployments.

Remplace progressivement les anciens pods par les nouveaux.

Une fois que les nouveaux Pods sont ready, on passe aux suivants.

TODO ajouter un sch√©ma

---

## `RollingUpdate` : mise √† jour progressive (2/2)

maxUnavailable :
- Nombre/pourcentage de pods qui peuvent √™tre indisponibles
- Plus √©lev√© = mise √† jour plus rapide, mais impact plus fort sur l'application

maxSurge :
- Nombre/pourcentage de pods suppl√©mentaires autoris√©s
- Plus √©lev√© = mise √† jour plus rapide, mais plus de ressources

---

## `RollingUpdate` : exemple de manifest

```yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: "20%"    # Peut √™tre un nombre ou un pourcentage
    maxSurge: "30%"          # Idem
```

Exemple avec 10 replicas => maxUnavailable=20%, maxSurge=30% :

- Minimum 8 pods disponibles (10 - 2 = 8)
- Maximum 13 pods au total (10 + 3 = 13)

---

## Strat√©gie `Recreate`

‚ö†Ô∏è **Interruption de service** mais √©vite les conflits de volumes RWO (ReadWriteOnce)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  strategy:
    type: Recreate  # Pas de RollingUpdate possible dans ce cas                           
  template:
    spec:
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: webapp-data  # RWO volume
```


---

## Rappel : rollback et historique

Vu dans le module 3 - quelques commandes essentielles :

```bash
# Voir l'historique des d√©ploiements
kubectl rollout history deployment/webapp

# Rollback vers la version pr√©c√©dente
kubectl rollout undo deployment/webapp

# Rollback vers une version sp√©cifique
kubectl rollout undo deployment/webapp --to-revision=2
```

---

## Blue/Green Deployment

Ce pattern n'existe pas nativement dans Kubernetes.

D√©ploiement en parall√®le de la version initiale (blue) une nouvelle version (green). Quand l'application (green) est pr√™te, on bascule tout le trafic.

**Avantages :** bascule instantan√©e, rollback rapide  
**Inconv√©nients :** double consommation de ressources, manuel (sauf outil tiers)

---

## Blue/Green Deployment : exemple

```bash
# D√©ployer la nouvelle version Green en parall√®le de Blue
kubectl apply -f webapp-green.yaml

# Modifier le service pour pointer vers Green
kubectl patch service webapp-service -p '{"spec":{"selector":{"version":"green"}}}'

# Supprimer l'ancienne version Blue
kubectl delete deployment webapp-blue
```

---

## Canary Deployment

Ce pattern n'existe pas nativement dans Kubernetes.

D√©ploiement progressif de la nouvelle version sur un sous-ensemble d'utilisateurs toujours plus grand.

**Avantages :** R√©duction des risques, validation progressive  
**Inconv√©nients :** n√©cessite monitoring fiable, manuel (sauf outil tiers)

---

## Canary Deployment : exemple

```bash
# D√©ployer la nouvelle version "canary", sur le m√™me Service
kubectl apply -f webapp-canary.yaml

# Augmenter progressivement le nombre de replicas "canary"
kubectl scale deployment webapp-canary --replicas=2
kubectl scale deployment webapp-stable --replicas=8

# Continuer la migration si les m√©triques sont OK
kubectl scale deployment webapp-canary --replicas=5
kubectl scale deployment webapp-stable --replicas=5

# Finaliser : version canary devient la version stable
kubectl delete deployment webapp-stable
```

---

## Outils avanc√©s pour Canary

[Flagger](https://flagger.app/) (avec service mesh tiers) :
- Canary automatique bas√© sur des m√©triques
- Rollback automatique en cas d'anomalie
- Int√©gration avec service mesh

[Argo Rollouts](https://argoproj.github.io/argo-rollouts/) :
- CRD pour des strat√©gies de d√©ploiement avanc√©es
- Analysis bas√©e sur Prometheus
- Int√©gration avec ingress controllers

---

<!-- _class: lead -->

# TP 4 : D√©ployer, surveiller et mettre √† jour une application

---

## Objectif du TP : un app pr√™te pour la production !

- Configurer health checks et ressources pour la production
- Mettre en place un HPA et tester le scaling
- Impl√©menter diff√©rentes strat√©gies de mise √† jour
- Surveiller et d√©boguer les d√©ploiements

*Instructions d√©taill√©es dans TP/module-4/*

---

<!-- _class: lead -->

## Questions ?

*Pr√™ts pour la s√©curit√© dans Kubernetes ?*

![bg fit right:40%](binaries/kubernetes_small.png)

---

## Bibliographie (1/3)

**Documentation officielle :**
- [Kubernetes Probes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)
- [HPA Walkthrough](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/)
- [Deployment Rolling Updates](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment)

---

## Bibliographie (2/3)

**Articles techniques :**
- [Prometheus Adapter - Deezer](https://deezer.io/optimizing-kubernetes-resources-with-horizontal-pod-autoscaling-via-custom-metrics-and-the-a76c1a66ff1c)
- [Sidecar Containers - blog.zwindler.fr](https://blog.zwindler.fr/2024/07/19/kubernetes-1-29-sidecar-containers/)
- [CPU Limits - Robusta](https://home.robusta.dev/blog/stop-using-cpu-limits)

---

## Bibliographie (3/3)

**Outils mentionn√©s :**
- [OpenTelemetry](https://opentelemetry.io/), [Prometheus](https://prometheus.io/), [Grafana](https://grafana.com/)
- [KEDA](https://keda.sh/), [KNative](https://knative.dev/), [Flagger](https://flagger.app/), [Argo Rollouts](https://argoproj.github.io/argo-rollouts/)